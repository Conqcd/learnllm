{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91055237",
   "metadata": {},
   "source": [
    "学习Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a73403",
   "metadata": {},
   "source": [
    "各类模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f64edde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant. Answer the question: What is Langchain?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "#这个模板完全就是一个字符串模板，使用`{}`来表示变量的位置。\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=\"You are a helpful assistant. Answer the question: {question}\",\n",
    ")\n",
    "prompt_template.format(question=\"What is Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329fe28f-0bbb-4138-9d32-d8de3ef1db18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "#这个模板用于聊天场景，支持多轮对话的格式化。方便你管理对话的上下文。\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "chat_prompt_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332c913",
   "metadata": {},
   "source": [
    "和上面类似，只不过把System合Human的包装变了种模样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8896f3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name, Alice?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "#这个模板用于人类消息的格式化，适合需要用户输入的场景。\n",
    "human_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "                ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"What is your name, {name}?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "human_prompt_template.format_messages(name=\"Alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9079c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='You are a helpful assistant. Answer the question: What is Langchain?', additional_kwargs={}, response_metadata={}, role='C')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "#这个模板用于聊天消息的格式化，支持更复杂的消息结构。单条信息\n",
    "chat_message_prompt_template = ChatMessagePromptTemplate.from_template(\n",
    "    role = 'C',\n",
    "    template=\"You are a helpful assistant. Answer the question: {question}\"\n",
    ")\n",
    "chat_message_prompt_template.format(question=\"What is Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dab414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='最小哈希MinHashing 最小哈希是我们流程的下一步，允许我们将稀疏向量转换为稠密向量。现在，作为预警 - 这个过程的这一部分最初可能看起来令人困惑 - 但一旦你理解它就非常简单了。我们有稀疏向量，我们所做的是为签名中的每个位置随机生成一个 minhash 函数（例如稠密向量）。因此，如果我们想创建一个包含 20 个数字的稠密向量/签名，我们将使用 20 个 minhash 函数。现在，这些 MinHash 函数只是数字的随机顺序——我们从 1 数到最终数字（即 len(vocab)）。由于这些数字的顺序是随机的，我们可能会发现数字1位于随机化 MinHash 函数的第 57 位。签名值是通过首先采用一个随机排列的计数向量（从 1 到 len(vocab)+1）来创建的，并在稀疏向量中找到与 1 对齐的最小数。', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='你好，我是AI助手。', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='总结一下这段对话的要点。用50个字。', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "# MessagesPlaceholder 用于在聊天模板中占位，表示将来会填充的消息。\n",
    "messages_placeholder = MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "human_message = \"总结一下这段对话的要点。用{word_count}个字。\"\n",
    "human_message_prompt_template = HumanMessagePromptTemplate.from_template(\n",
    "    human_message\n",
    ")\n",
    "\n",
    "chat_promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        messages_placeholder,\n",
    "        human_message_prompt_template\n",
    "    ]\n",
    ")\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "# 创建一个包含多条消息的列表\n",
    "messages = [\n",
    "    HumanMessage(content=\"最小哈希MinHashing \"\n",
    "        \"最小哈希是我们流程的下一步，允许我们将稀疏向量转换为稠密向量。现在，作为预警 - 这个过程的这一部分最初可能看起来令人困惑 - 但一旦你理解它就非常简单了。\"\n",
    "        \"我们有稀疏向量，我们所做的是为签名中的每个位置随机生成一个 minhash 函数（例如稠密向量）。\"\n",
    "        \"因此，如果我们想创建一个包含 20 个数字的稠密向量/签名，我们将使用 20 个 minhash 函数。\"\n",
    "        \"现在，这些 MinHash 函数只是数字的随机顺序——我们从 1 数到最终数字（即 len(vocab)）。由于这些数字的顺序是随机的，我们可能会发现数字1位于随机化 MinHash 函数的第 57 位。\"\n",
    "        \"签名值是通过首先采用一个随机排列的计数向量（从 1 到 len(vocab)+1）来创建的，并在稀疏向量中找到与 1 对齐的最小数。\"\n",
    "        ),\n",
    "    AIMessage(content=\"你好，我是AI助手。\")\n",
    "]\n",
    "\n",
    "# 使用 ChatPromptTemplate 格式化消息\n",
    "formatted_messages = chat_promt.format_messages(messages=messages, word_count=50)\n",
    "formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2f4a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What is Langchain?\\nAI: Langchain is a framework for building applications with LLMs.\\nHuman: How does it work?\\nAI: It works by chaining together components that interact with LLMs.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=[\n",
    "        {\n",
    "            \"input\": \"What is Langchain?\",\n",
    "            \"output\": \"Langchain is a framework for building applications with LLMs.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"How does it work?\",\n",
    "            \"output\": \"It works by chaining together components that interact with LLMs.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "example_prompt.format()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca35b6",
   "metadata": {},
   "source": [
    "使用大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab09e7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Tongyi\n  Value error, Did not find dashscope_api_key, please add an environment variable `DASHSCOPE_API_KEY` which contains it, or pass `dashscope_api_key` as a named parameter. [type=value_error, input_value={'name': None, 'cache': N...alse, 'max_retries': 10}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tongyi\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m Tongyi()\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    260\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Tongyi\n  Value error, Did not find dashscope_api_key, please add an environment variable `DASHSCOPE_API_KEY` which contains it, or pass `dashscope_api_key` as a named parameter. [type=value_error, input_value={'name': None, 'cache': N...alse, 'max_retries': 10}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Tongyi\n",
    "\n",
    "llm = Tongyi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb8b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

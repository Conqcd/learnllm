{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b788d4e6",
   "metadata": {},
   "source": [
    "检索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153835e",
   "metadata": {},
   "source": [
    "Text文本方式读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931770c1-fad8-42ab-b617-1517acb54096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/sql.md'}, page_content=\"## 创建表\\n\\n```sql\\n# 分区表\\ncreate table test_t2(words string,frequency string) partitioned by (partdate string) row format delimited fields terminated by ',';\\n\\n# orc表\\nCREATE TABLE IF NOT EXISTS bank.account_orc (\\n  `id_card` int,\\n  `tran_time` string,\\n  `name` string,\\n  `cash` int\\n  )\\nstored as orc;\\n```\\n\\n# 插入数据\\n\\n```sql\\ninsert into tablename values('col1', 'col2');\\n\\n\\nINSERT INTO table_name (column1, column2, column3)\\nVALUES\\n(value1, value2, value3),\\n(value4, value5, value6),\\n(value7, value8, value9);\\n\\n\\nINSERT OVERWRITE TABLE tb\\nselect * from tb2\\n;\\n```\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./examples/sql.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9028c1d",
   "metadata": {},
   "source": [
    "CSV文件的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b372a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/test.csv', 'row': 0}, page_content='id: 1\\nname: 张三\\ndegree: 本科'),\n",
       " Document(metadata={'source': './examples/test.csv', 'row': 1}, page_content='id: 2\\nname: 李四\\ndegree: 硕士')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path='./examples/test.csv')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8b89ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '1', 'row': 0}, page_content='id: 1\\nname: 张三\\ndegree: 本科'),\n",
       " Document(metadata={'source': '2', 'row': 1}, page_content='id: 2\\nname: 李四\\ndegree: 硕士')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path='./examples/no_fields_name.csv', csv_args={\n",
    "    'delimiter': ',',\n",
    "    'quotechar': '\"',\n",
    "    'fieldnames': ['id', 'name', 'degree']\n",
    "    }, \n",
    "    source_column='id'\n",
    ")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838eee",
   "metadata": {},
   "source": [
    "pdf读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba75a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous trailer cannot be read: (\"invalid literal for int() with base 10: b'/Root'\",)\n",
      "Object 14 0 found\n",
      "Object 3 0 found\n",
      "Object 2 0 found\n",
      "Object 5 0 found\n",
      "Object 7 0 found\n",
      "Object 21 0 found\n",
      "Object 20 0 found\n",
      "Object 22 0 found\n",
      "Object 8 0 found\n",
      "Object 25 0 found\n",
      "Object 9 0 found\n",
      "Object 27 0 found\n",
      "Object 10 0 found\n",
      "Object 30 0 found\n",
      "Object 29 0 found\n",
      "Object 31 0 found\n",
      "Object 12 0 found\n",
      "Object 35 0 found\n",
      "Object 34 0 found\n",
      "Object 4 0 found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'examples/sql.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"创建表  \\n插⼊数据  \\n# 分区表\\ncreate table test_t2(words string,frequency string) partitioned by (partdate string) row \\nformat delimited fields terminated by ',';\\n# orc表\\nCREATE TABLE IF NOT EXISTS bank.account_orc (\\n \\xa0`id_card` int,\\n \\xa0`tran_time` string,\\n \\xa0`name` string,\\n \\xa0`cash` int\\n \\xa0)\\nstored as orc;\\ninsert into tablename values('col1', 'col2');\\nINSERT INTO table_name (column1, column2, column3)\\nVALUES\\n(value1, value2, value3),\\n(value4, value5, value6),\\n(value7, value8, value9);\\nINSERT OVERWRITE TABLE tb\\nselect * from tb2\\n;\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"examples/sql.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a767f",
   "metadata": {},
   "source": [
    "loader可以自己定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "    # 实现 lazy_load 和 alazy_load 方法，分别为load和异步load方式\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                \n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "    # alazy_load is OPTIONAL.\n",
    "    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "    async def alazy_load(\n",
    "        self,\n",
    "    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "        # Requires aiofiles\n",
    "        # Install with `pip install aiofiles`\n",
    "        # https://github.com/Tinche/aiofiles\n",
    "        import aiofiles\n",
    "\n",
    "        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            async for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7a1a9",
   "metadata": {},
   "source": [
    "文本分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06b716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to'),\n",
       " Document(metadata={}, page_content='to chunk up. It is the example text'),\n",
       " Document(metadata={}, page_content='text for this exercise')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651491a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is\\n the text I would'),\n",
       " Document(metadata={}, page_content='like to chunk up.It is the example text for this exercise')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is\\n the text I would\\n like to chunk up.It is the example text for this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2048852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is\\n\\n the'),\n",
       " Document(metadata={}, page_content='text\\n I would like'),\n",
       " Document(metadata={}, page_content='to chunk up. It is the example'),\n",
       " Document(metadata={}, page_content='text'),\n",
       " Document(metadata={}, page_content='for'),\n",
       " Document(metadata={}, page_content='this exercise')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#首选分隔符是\\n\\n，然后是\\n，接着是空格，最后是空字符串\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is\\n\\n the\\n\\n text\\n I would like\\n to chunk up. It is the example text\\n for\\n\\n this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54ae3641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='text\\n for\\n\\n this exercise')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"text\\n for\\n\\n this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5460b64",
   "metadata": {},
   "source": [
    "以上能看出，先对\\n\\n分割，然后组合chunk，然后对剩余长于chunk_size的进行\\n分割再组合，然后是‘ ’，最后再分割‘’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e10b9",
   "metadata": {},
   "source": [
    "对代码进行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02cdad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91cca545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6887d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \\n#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'}, page_content='Implementations of Markdown are available for over a dozen programming languages.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9e0609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'}, page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a44c3",
   "metadata": {},
   "source": [
    "可以看出上面再次分割了内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ed70b",
   "metadata": {},
   "source": [
    "根据语义分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b35248",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../state_of_the_union.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This is a long document we can split up.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../state_of_the_union.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     state_of_the_union \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m SemanticChunker(OpenAIEmbeddings())\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../state_of_the_union.txt'"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# This is a long document we can split up.\n",
    "with open(\"../../state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "    \n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "docs = text_splitter.create_documents([state_of_the_union])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ef274",
   "metadata": {},
   "source": [
    "嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bbb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

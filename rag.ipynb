{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b788d4e6",
   "metadata": {},
   "source": [
    "检索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153835e",
   "metadata": {},
   "source": [
    "Text文本方式读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931770c1-fad8-42ab-b617-1517acb54096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/sql.md'}, page_content=\"## 创建表\\n\\n```sql\\n# 分区表\\ncreate table test_t2(words string,frequency string) partitioned by (partdate string) row format delimited fields terminated by ',';\\n\\n# orc表\\nCREATE TABLE IF NOT EXISTS bank.account_orc (\\n  `id_card` int,\\n  `tran_time` string,\\n  `name` string,\\n  `cash` int\\n  )\\nstored as orc;\\n```\\n\\n# 插入数据\\n\\n```sql\\ninsert into tablename values('col1', 'col2');\\n\\n\\nINSERT INTO table_name (column1, column2, column3)\\nVALUES\\n(value1, value2, value3),\\n(value4, value5, value6),\\n(value7, value8, value9);\\n\\n\\nINSERT OVERWRITE TABLE tb\\nselect * from tb2\\n;\\n```\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./examples/sql.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9028c1d",
   "metadata": {},
   "source": [
    "CSV文件的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b372a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/test.csv', 'row': 0}, page_content='id: 1\\nname: 张三\\ndegree: 本科'),\n",
       " Document(metadata={'source': './examples/test.csv', 'row': 1}, page_content='id: 2\\nname: 李四\\ndegree: 硕士')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path='./examples/test.csv')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8b89ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '1', 'row': 0}, page_content='id: 1\\nname: 张三\\ndegree: 本科'),\n",
       " Document(metadata={'source': '2', 'row': 1}, page_content='id: 2\\nname: 李四\\ndegree: 硕士')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path='./examples/no_fields_name.csv', csv_args={\n",
    "    'delimiter': ',',\n",
    "    'quotechar': '\"',\n",
    "    'fieldnames': ['id', 'name', 'degree']\n",
    "    }, \n",
    "    source_column='id'\n",
    ")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838eee",
   "metadata": {},
   "source": [
    "pdf读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba75a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous trailer cannot be read: (\"invalid literal for int() with base 10: b'/Root'\",)\n",
      "Object 14 0 found\n",
      "Object 3 0 found\n",
      "Object 2 0 found\n",
      "Object 5 0 found\n",
      "Object 7 0 found\n",
      "Object 21 0 found\n",
      "Object 20 0 found\n",
      "Object 22 0 found\n",
      "Object 8 0 found\n",
      "Object 25 0 found\n",
      "Object 9 0 found\n",
      "Object 27 0 found\n",
      "Object 10 0 found\n",
      "Object 30 0 found\n",
      "Object 29 0 found\n",
      "Object 31 0 found\n",
      "Object 12 0 found\n",
      "Object 35 0 found\n",
      "Object 34 0 found\n",
      "Object 4 0 found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'examples/sql.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"创建表  \\n插⼊数据  \\n# 分区表\\ncreate table test_t2(words string,frequency string) partitioned by (partdate string) row \\nformat delimited fields terminated by ',';\\n# orc表\\nCREATE TABLE IF NOT EXISTS bank.account_orc (\\n \\xa0`id_card` int,\\n \\xa0`tran_time` string,\\n \\xa0`name` string,\\n \\xa0`cash` int\\n \\xa0)\\nstored as orc;\\ninsert into tablename values('col1', 'col2');\\nINSERT INTO table_name (column1, column2, column3)\\nVALUES\\n(value1, value2, value3),\\n(value4, value5, value6),\\n(value7, value8, value9);\\nINSERT OVERWRITE TABLE tb\\nselect * from tb2\\n;\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"examples/sql.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a767f",
   "metadata": {},
   "source": [
    "loader可以自己定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c7915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "    # 实现 lazy_load 和 alazy_load 方法，分别为load和异步load方式\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                \n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "    # alazy_load is OPTIONAL.\n",
    "    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "    async def alazy_load(\n",
    "        self,\n",
    "    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "        # Requires aiofiles\n",
    "        # Install with `pip install aiofiles`\n",
    "        # https://github.com/Tinche/aiofiles\n",
    "        import aiofiles\n",
    "\n",
    "        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            async for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7a1a9",
   "metadata": {},
   "source": [
    "文本分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac06b716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to'),\n",
       " Document(metadata={}, page_content='to chunk up. It is the example text'),\n",
       " Document(metadata={}, page_content='text for this exercise')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651491a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is\\n the text I would'),\n",
       " Document(metadata={}, page_content='like to chunk up.It is the example text for this exercise')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is\\n the text I would\\n like to chunk up.It is the example text for this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2048852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is\\n\\n the'),\n",
       " Document(metadata={}, page_content='text\\n I would like'),\n",
       " Document(metadata={}, page_content='to chunk up. It is the example'),\n",
       " Document(metadata={}, page_content='text'),\n",
       " Document(metadata={}, page_content='for'),\n",
       " Document(metadata={}, page_content='this exercise')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#首选分隔符是\\n\\n，然后是\\n，接着是空格，最后是空字符串\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is\\n\\n the\\n\\n text\\n I would like\\n to chunk up. It is the example text\\n for\\n\\n this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ae3641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='text\\n for\\n\\n this exercise')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"text\\n for\\n\\n this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5460b64",
   "metadata": {},
   "source": [
    "以上能看出，先对\\n\\n分割，然后组合chunk，然后对剩余长于chunk_size的进行\\n分割再组合，然后是‘ ’，最后再分割‘’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e10b9",
   "metadata": {},
   "source": [
    "对代码进行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cdad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91cca545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6887d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \\n#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'}, page_content='Implementations of Markdown are available for over a dozen programming languages.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9e0609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'}, page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a44c3",
   "metadata": {},
   "source": [
    "可以看出上面再次分割了内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ed70b",
   "metadata": {},
   "source": [
    "根据语义分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b35248",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../state_of_the_union.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This is a long document we can split up.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../state_of_the_union.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     state_of_the_union \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m SemanticChunker(OpenAIEmbeddings())\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../state_of_the_union.txt'"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# This is a long document we can split up.\n",
    "with open(\"../../state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "    \n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "#根据语义进行分割，本质上要用embeddings模型来计算相似度，高于某个阈值的邻域chunk可以合并到一起"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ef274",
   "metadata": {},
   "source": [
    "嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195bbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "# Ensure you have the environment variable set for the API key\n",
    "os.environ[\"OpenAI_API_BASE\"] = \"https://api.zhizengzeng.com/v1\"\n",
    "os.environ[\"OpenAI_API_KEY\"] = \"sk-zk22167acdc980aa5bd5dd04774d59f30f0684991a3a2fb2\"\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    chunk_size=1000,  # Optional, default is 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d277fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "        \"你好吗\",\n",
    "        \"你的名字是什么\",\n",
    "        \"我的肚子好痛啊\",\n",
    "        \"肠胃不舒服\",\n",
    "        \"我在吃东西\"\n",
    "    ]\n",
    "embeddings = embeddings_model.embed_documents(texts)\n",
    "\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df572d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"我的肚子好痛啊\"与\"你好吗\"的语义相似度为：0.3594570606893717\n",
      "\"我的肚子好痛啊\"与\"你的名字是什么\"的语义相似度为：0.23817256999431716\n",
      "\"我的肚子好痛啊\"与\"我的肚子好痛啊\"的语义相似度为：1.0\n",
      "\"我的肚子好痛啊\"与\"肠胃不舒服\"的语义相似度为：0.55998006426397\n",
      "\"我的肚子好痛啊\"与\"我在吃东西\"的语义相似度为：0.4246934047589308\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.asarray(x)\n",
    "    norms = np.sum(np.multiply(x, x))\n",
    "    norms = np.sqrt(norms)\n",
    "    return x / norms\n",
    "\n",
    "for i in range(5):\n",
    "    similarity = np.dot(normalize(embeddings[2]), normalize(embeddings[i]))\n",
    "    print(f'\"{texts[2]}\"与\"{texts[i]}\"的语义相似度为：{similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f2a91",
   "metadata": {},
   "source": [
    "Embedding也可以Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5fd677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, store, namespace=embeddings_model.tiktoken_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2c5ff2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\embeddings\\cache.py:125\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of texts.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    The method first checks the cache for the embeddings.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m        A list of embeddings for the given texts.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     vectors: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_embedding_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     all_missing_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    129\u001b[0m         i \u001b[38;5;28;01mfor\u001b[39;00m i, vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vectors) \u001b[38;5;28;01mif\u001b[39;00m vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     ]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m missing_indices \u001b[38;5;129;01min\u001b[39;00m batch_iterate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, all_missing_indices):\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\storage\\encoder_backed.py:66\u001b[0m, in \u001b[0;36mEncoderBackedStore.mget\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmget\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: Sequence[K]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Optional[V]]:\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     encoded_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     67\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mmget(encoded_keys)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_deserializer(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[0;32m     71\u001b[0m     ]\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\storage\\encoder_backed.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmget\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: Sequence[K]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Optional[V]]:\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     encoded_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[0;32m     67\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mmget(encoded_keys)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_deserializer(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[0;32m     71\u001b[0m     ]\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\embeddings\\cache.py:36\u001b[0m, in \u001b[0;36m_key_encoder\u001b[1;34m(key, namespace)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_key_encoder\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, namespace: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode a key.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnamespace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_hash_string_to_uuid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cached_embedder.embed_documents(texts)\n",
    "#两次调用会让第一次调用的结果被缓存起来，第二次调用会直接从缓存中读取，从而加速第二次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3416d5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CacheBackedEmbeddings.embed_documents of <langchain.embeddings.cache.CacheBackedEmbeddings object at 0x0000019FEFB37990>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, store, namespace=embeddings_model.tiktoken_model_name\n",
    ")\n",
    "\n",
    "cached_embedder.embed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c11a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The RedisStore requires the redis library to be installed. pip install redis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_community\\storage\\redis.py:60\u001b[0m, in \u001b[0;36mRedisStore.__init__\u001b[1;34m(self, client, redis_url, client_kwargs, ttl, namespace)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Redis\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'redis'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RedisStore\n\u001b[1;32m----> 3\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43mRedisStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m cached_embedder \u001b[38;5;241m=\u001b[39m CacheBackedEmbeddings\u001b[38;5;241m.\u001b[39mfrom_bytes_store(\n\u001b[0;32m      6\u001b[0m     embeddings_model, store, namespace\u001b[38;5;241m=\u001b[39membeddings_model\u001b[38;5;241m.\u001b[39mtiktoken_model_name\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m cached_embedder\u001b[38;5;241m.\u001b[39membed_documents\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_community\\storage\\redis.py:62\u001b[0m, in \u001b[0;36mRedisStore.__init__\u001b[1;34m(self, client, redis_url, client_kwargs, ttl, namespace)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Redis\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe RedisStore requires the redis library to be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install redis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mand\u001b[39;00m (redis_url \u001b[38;5;129;01mor\u001b[39;00m client_kwargs):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither a Redis client or a redis_url with optional client_kwargs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be provided, but not both.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: The RedisStore requires the redis library to be installed. pip install redis"
     ]
    }
   ],
   "source": [
    "from langchain.storage import RedisStore\n",
    "#需要本地注册Redis才能跑\n",
    "store = RedisStore()\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, store, namespace=embeddings_model.tiktoken_model_name\n",
    ")\n",
    "\n",
    "cached_embedder.embed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d6b20",
   "metadata": {},
   "source": [
    "上面三种方法去缓存embedding，本地文件、内存、Redis数据库，Redis数据库是内存数据库，其实也可以写到磁盘以及分布式，所以能力更强一些"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e5b31",
   "metadata": {},
   "source": [
    "如何使用Faiss检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0020ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69487e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 109, which is longer than the specified 50\n",
      "Created a chunk of size 65, which is longer than the specified 50\n",
      "Created a chunk of size 143, which is longer than the specified 50\n",
      "Created a chunk of size 833, which is longer than the specified 50\n",
      "Created a chunk of size 263, which is longer than the specified 50\n",
      "Created a chunk of size 304, which is longer than the specified 50\n",
      "Created a chunk of size 609, which is longer than the specified 50\n",
      "Created a chunk of size 174, which is longer than the specified 50\n",
      "Created a chunk of size 449, which is longer than the specified 50\n",
      "Created a chunk of size 280, which is longer than the specified 50\n",
      "Created a chunk of size 325, which is longer than the specified 50\n",
      "Created a chunk of size 560, which is longer than the specified 50\n",
      "Created a chunk of size 318, which is longer than the specified 50\n",
      "Created a chunk of size 68, which is longer than the specified 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/rag.txt'}, page_content='2024年普通高等学校招生全国统一考试（简称：2024年全国高考），是中华人民共和国合格的高中毕业生或具有同等学力的考生参加的选拔性考试 [1-2]。2024年报名人数1342万人，比2023年增加51万人 [21]。'),\n",
       " Document(metadata={'source': './examples/rag.txt'}, page_content='2024年高考是黑龙江、甘肃、吉林、安徽、江西、贵州、广西7个省份（中国第四批高考综合改革省份）的第一届落地实施的新高考。 [3]')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('./examples/rag.txt').load()\n",
    "text_splitter = CharacterTextSplitter(separator='\\n\\n\\n', chunk_size=50, chunk_overlap=4)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490f3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01620c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、在哪里可以了解高考成绩、志愿填报时间和方式、各高校招生计划、往年录取参考等志愿填报权威信息？\n",
      "各省级教育行政部门或招生考试机构官方网站、微信公众号等权威渠道都会公布今年高考各阶段工作时间安排，包括高考成绩公布时间和查询方式、志愿填报时间，以及今年各高校招生计划、往年录取情况参考等权威信息。考生和家长要及时关注本地官方权威渠道发布的消息内容。\n",
      "考生高考志愿是高校录取的重要依据，请广大考生务必按照省级招生考试机构相关要求按时完成志愿填报。前期，教育部已会同有关部门协调互联网平台对省级招生考试机构和高校的官方网站、微信公众号等进行了权威标识，请广大考生在信息查询时认准官方权威渠道，切勿轻信网络不实信息。\n"
     ]
    }
   ],
   "source": [
    "query = \"哪里可以了解高考成绩\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb9f092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7b4b322f-44ee-4a23-b61f-6de115a4c8fc', metadata={'source': './examples/rag.txt'}, page_content='一、在哪里可以了解高考成绩、志愿填报时间和方式、各高校招生计划、往年录取参考等志愿填报权威信息？\\n各省级教育行政部门或招生考试机构官方网站、微信公众号等权威渠道都会公布今年高考各阶段工作时间安排，包括高考成绩公布时间和查询方式、志愿填报时间，以及今年各高校招生计划、往年录取情况参考等权威信息。考生和家长要及时关注本地官方权威渠道发布的消息内容。\\n考生高考志愿是高校录取的重要依据，请广大考生务必按照省级招生考试机构相关要求按时完成志愿填报。前期，教育部已会同有关部门协调互联网平台对省级招生考试机构和高校的官方网站、微信公众号等进行了权威标识，请广大考生在信息查询时认准官方权威渠道，切勿轻信网络不实信息。'),\n",
       " Document(id='83b2b115-118b-4f03-a67e-7d5df0d9547a', metadata={'source': './examples/rag.txt'}, page_content='二、高考志愿填报咨询有哪些公共服务？\\n教育部高度重视高考志愿填报咨询服务工作，指导各地建立了招生考试机构、高校、中学多方面志愿填报咨询公共服务体系。在教育部层面，首次在“阳光高考平台”推出免费的阳光志愿信息服务系统，将海量数据系统集成，进行个性化匹配推荐，从专业、就业、职业等多方面帮助考生了解学校和专业。同时还将举办“高考志愿填报云咨询周”活动，组织各省级招生考试机构和高校通过文字问答、视频直播等方式，为全国考生和家长提供志愿填报咨询指导公益服务。在各地层面，地方招生考试机构将通过印发志愿填报指导材料、推出志愿填报参考信息服务系统等多种方式为考生提供填报志愿所需的必要信息和服务，包括今年高校招生计划、高校近年录取情况、志愿填报办法和招生录取政策、考生须知等，并通过电视（电台）政策宣讲、线上直播咨询等方式为考生解疑释惑。在学校层面，招生高校会组织开展线上线下咨询活动和在线直播等活动，解读学校招生章程、招生办法和往年录取参考信息，提供各类志愿填报咨询服务；中学会面向考生和家长进行志愿填报培训，及时提供相关部门和高校权威宣传解读资料、发布渠道、发布安排等信息，并组织教师为考生和家长提供针对性地指导服务。\\n考生可以通过所在地省级招生考试机构和各高校官方网站、官方微信公众号或编发的志愿填报指导材料等渠道查询所需参考信息，也可通过拨打当地招生考试机构、高校和中学开通的咨询电话或线上咨询等方式了解相关政策和信息。'),\n",
       " Document(id='b0c3d728-6a51-480c-ba71-71d0f91c61ee', metadata={'source': './examples/rag.txt'}, page_content='三、高校招生章程有什么作用，如何查询？\\n高校招生章程由学校依据相关法律规定和国家招生政策制定，是学校开展招生工作的依据。考生在填报志愿前，应仔细查阅拟报考高校的招生章程，全面了解高校招生办法和相关招生要求。\\n主要查询途径有：中国高等教育学生信息网的“阳光高考”信息平台（https://gaokao.chsi.com.cn）；各高校官方招生网站等。'),\n",
       " Document(id='1c2e31c1-d99b-4383-a528-015eeee9e75f', metadata={'source': './examples/rag.txt'}, page_content='十、录取通知书何时能收到？\\n高校一般会在录取结束后一周左右向录取新生寄发录取通知书。若考生在省级招生考试机构或高校官方网站上查询到了录取结果，一直没有收到录取通知书，可及时联系录取高校公布的招生咨询电话查询本人录取通知书邮寄情况。')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ba4ac",
   "metadata": {},
   "source": [
    "也可以直接用向量检索，道理是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e043d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、在哪里可以了解高考成绩、志愿填报时间和方式、各高校招生计划、往年录取参考等志愿填报权威信息？\n",
      "各省级教育行政部门或招生考试机构官方网站、微信公众号等权威渠道都会公布今年高考各阶段工作时间安排，包括高考成绩公布时间和查询方式、志愿填报时间，以及今年各高校招生计划、往年录取情况参考等权威信息。考生和家长要及时关注本地官方权威渠道发布的消息内容。\n",
      "考生高考志愿是高校录取的重要依据，请广大考生务必按照省级招生考试机构相关要求按时完成志愿填报。前期，教育部已会同有关部门协调互联网平台对省级招生考试机构和高校的官方网站、微信公众号等进行了权威标识，请广大考生在信息查询时认准官方权威渠道，切勿轻信网络不实信息。\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = embeddings_model.embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6672e7",
   "metadata": {},
   "source": [
    "最大边际相关性，为了衡量多样性和相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abdc781d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7b4b322f-44ee-4a23-b61f-6de115a4c8fc', metadata={'source': './examples/rag.txt'}, page_content='一、在哪里可以了解高考成绩、志愿填报时间和方式、各高校招生计划、往年录取参考等志愿填报权威信息？\\n各省级教育行政部门或招生考试机构官方网站、微信公众号等权威渠道都会公布今年高考各阶段工作时间安排，包括高考成绩公布时间和查询方式、志愿填报时间，以及今年各高校招生计划、往年录取情况参考等权威信息。考生和家长要及时关注本地官方权威渠道发布的消息内容。\\n考生高考志愿是高校录取的重要依据，请广大考生务必按照省级招生考试机构相关要求按时完成志愿填报。前期，教育部已会同有关部门协调互联网平台对省级招生考试机构和高校的官方网站、微信公众号等进行了权威标识，请广大考生在信息查询时认准官方权威渠道，切勿轻信网络不实信息。'),\n",
       " Document(id='eb581920-cec4-47e9-9457-75ba318914e0', metadata={'source': './examples/rag.txt'}, page_content='2024年高考是黑龙江、甘肃、吉林、安徽、江西、贵州、广西7个省份（中国第四批高考综合改革省份）的第一届落地实施的新高考。 [3]'),\n",
       " Document(id='cc970c86-6027-4529-988e-e64352e0d048', metadata={'source': './examples/rag.txt'}, page_content='2024年普通高等学校招生全国统一考试（简称：2024年全国高考），是中华人民共和国合格的高中毕业生或具有同等学力的考生参加的选拔性考试 [1-2]。2024年报名人数1342万人，比2023年增加51万人 [21]。'),\n",
       " Document(id='1c2e31c1-d99b-4383-a528-015eeee9e75f', metadata={'source': './examples/rag.txt'}, page_content='十、录取通知书何时能收到？\\n高校一般会在录取结束后一周左右向录取新生寄发录取通知书。若考生在省级招生考试机构或高校官方网站上查询到了录取结果，一直没有收到录取通知书，可及时联系录取高校公布的招生咨询电话查询本人录取通知书邮寄情况。')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"哪里可以了解高考成绩\"\n",
    "docs = db.max_marginal_relevance_search(query)\n",
    "docs\n",
    "#在公式里，除了query，还有对搜索结果chunk和其他chunk的相似度，然后一个λ作为衡量多样性和相对性，1则是完全看chunk和q的相似度，0则是完全看chunk之间的相似度。\n",
    "#还有一个参数k，表示返回的文档数量，默认为4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee318bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7b4b322f-44ee-4a23-b61f-6de115a4c8fc', metadata={'source': './examples/rag.txt'}, page_content='一、在哪里可以了解高考成绩、志愿填报时间和方式、各高校招生计划、往年录取参考等志愿填报权威信息？\\n各省级教育行政部门或招生考试机构官方网站、微信公众号等权威渠道都会公布今年高考各阶段工作时间安排，包括高考成绩公布时间和查询方式、志愿填报时间，以及今年各高校招生计划、往年录取情况参考等权威信息。考生和家长要及时关注本地官方权威渠道发布的消息内容。\\n考生高考志愿是高校录取的重要依据，请广大考生务必按照省级招生考试机构相关要求按时完成志愿填报。前期，教育部已会同有关部门协调互联网平台对省级招生考试机构和高校的官方网站、微信公众号等进行了权威标识，请广大考生在信息查询时认准官方权威渠道，切勿轻信网络不实信息。'),\n",
       " Document(id='83b2b115-118b-4f03-a67e-7d5df0d9547a', metadata={'source': './examples/rag.txt'}, page_content='二、高考志愿填报咨询有哪些公共服务？\\n教育部高度重视高考志愿填报咨询服务工作，指导各地建立了招生考试机构、高校、中学多方面志愿填报咨询公共服务体系。在教育部层面，首次在“阳光高考平台”推出免费的阳光志愿信息服务系统，将海量数据系统集成，进行个性化匹配推荐，从专业、就业、职业等多方面帮助考生了解学校和专业。同时还将举办“高考志愿填报云咨询周”活动，组织各省级招生考试机构和高校通过文字问答、视频直播等方式，为全国考生和家长提供志愿填报咨询指导公益服务。在各地层面，地方招生考试机构将通过印发志愿填报指导材料、推出志愿填报参考信息服务系统等多种方式为考生提供填报志愿所需的必要信息和服务，包括今年高校招生计划、高校近年录取情况、志愿填报办法和招生录取政策、考生须知等，并通过电视（电台）政策宣讲、线上直播咨询等方式为考生解疑释惑。在学校层面，招生高校会组织开展线上线下咨询活动和在线直播等活动，解读学校招生章程、招生办法和往年录取参考信息，提供各类志愿填报咨询服务；中学会面向考生和家长进行志愿填报培训，及时提供相关部门和高校权威宣传解读资料、发布渠道、发布安排等信息，并组织教师为考生和家长提供针对性地指导服务。\\n考生可以通过所在地省级招生考试机构和各高校官方网站、官方微信公众号或编发的志愿填报指导材料等渠道查询所需参考信息，也可通过拨打当地招生考试机构、高校和中学开通的咨询电话或线上咨询等方式了解相关政策和信息。')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "retriever.invoke(\"哪里可以了解高考成绩\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddaf94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

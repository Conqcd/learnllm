{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b788d4e6",
   "metadata": {},
   "source": [
    "检索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153835e",
   "metadata": {},
   "source": [
    "Text文本方式读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931770c1-fad8-42ab-b617-1517acb54096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/sql.md'}, page_content=\"## 创建表\\n\\n```sql\\n# 分区表\\ncreate table test_t2(words string,frequency string) partitioned by (partdate string) row format delimited fields terminated by ',';\\n\\n# orc表\\nCREATE TABLE IF NOT EXISTS bank.account_orc (\\n  `id_card` int,\\n  `tran_time` string,\\n  `name` string,\\n  `cash` int\\n  )\\nstored as orc;\\n```\\n\\n# 插入数据\\n\\n```sql\\ninsert into tablename values('col1', 'col2');\\n\\n\\nINSERT INTO table_name (column1, column2, column3)\\nVALUES\\n(value1, value2, value3),\\n(value4, value5, value6),\\n(value7, value8, value9);\\n\\n\\nINSERT OVERWRITE TABLE tb\\nselect * from tb2\\n;\\n```\")]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./examples/sql.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9028c1d",
   "metadata": {},
   "source": [
    "CSV文件的读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b372a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './examples/test.csv', 'row': 0}, page_content='id: 1\\nname: 张三\\ndegree: 本科'),\n",
       " Document(metadata={'source': './examples/test.csv', 'row': 1}, page_content='id: 2\\nname: 李四\\ndegree: 硕士')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "\n",
    "loader = CSVLoader(file_path='./examples/test.csv')\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8b89ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '1', 'row': 0}, page_content='id: 1\\nname: 张三\\ndegree: 本科'),\n",
       " Document(metadata={'source': '2', 'row': 1}, page_content='id: 2\\nname: 李四\\ndegree: 硕士')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path='./examples/no_fields_name.csv', csv_args={\n",
    "    'delimiter': ',',\n",
    "    'quotechar': '\"',\n",
    "    'fieldnames': ['id', 'name', 'degree']\n",
    "    }, \n",
    "    source_column='id'\n",
    ")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838eee",
   "metadata": {},
   "source": [
    "pdf读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba75a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous trailer cannot be read: (\"invalid literal for int() with base 10: b'/Root'\",)\n",
      "Object 14 0 found\n",
      "Object 3 0 found\n",
      "Object 2 0 found\n",
      "Object 5 0 found\n",
      "Object 7 0 found\n",
      "Object 21 0 found\n",
      "Object 20 0 found\n",
      "Object 22 0 found\n",
      "Object 8 0 found\n",
      "Object 25 0 found\n",
      "Object 9 0 found\n",
      "Object 27 0 found\n",
      "Object 10 0 found\n",
      "Object 30 0 found\n",
      "Object 29 0 found\n",
      "Object 31 0 found\n",
      "Object 12 0 found\n",
      "Object 35 0 found\n",
      "Object 34 0 found\n",
      "Object 4 0 found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'examples/sql.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"创建表  \\n插⼊数据  \\n# 分区表\\ncreate table test_t2(words string,frequency string) partitioned by (partdate string) row \\nformat delimited fields terminated by ',';\\n# orc表\\nCREATE TABLE IF NOT EXISTS bank.account_orc (\\n \\xa0`id_card` int,\\n \\xa0`tran_time` string,\\n \\xa0`name` string,\\n \\xa0`cash` int\\n \\xa0)\\nstored as orc;\\ninsert into tablename values('col1', 'col2');\\nINSERT INTO table_name (column1, column2, column3)\\nVALUES\\n(value1, value2, value3),\\n(value4, value5, value6),\\n(value7, value8, value9);\\nINSERT OVERWRITE TABLE tb\\nselect * from tb2\\n;\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"examples/sql.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a767f",
   "metadata": {},
   "source": [
    "loader可以自己定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c7915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "    # 实现 lazy_load 和 alazy_load 方法，分别为load和异步load方式\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                \n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "    # alazy_load is OPTIONAL.\n",
    "    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!\n",
    "    async def alazy_load(\n",
    "        self,\n",
    "    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"An async lazy loader that reads a file line by line.\"\"\"\n",
    "        # Requires aiofiles\n",
    "        # Install with `pip install aiofiles`\n",
    "        # https://github.com/Tinche/aiofiles\n",
    "        import aiofiles\n",
    "\n",
    "        async with aiofiles.open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            async for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7a1a9",
   "metadata": {},
   "source": [
    "文本分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac06b716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is the text I would like to'),\n",
       " Document(metadata={}, page_content='to chunk up. It is the example text'),\n",
       " Document(metadata={}, page_content='text for this exercise')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is the text I would like to chunk up. It is the example text for this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651491a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is\\n the text I would'),\n",
       " Document(metadata={}, page_content='like to chunk up.It is the example text for this exercise')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is\\n the text I would\\n like to chunk up.It is the example text for this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is\\n\\n the'),\n",
       " Document(metadata={}, page_content='text\\n I would like'),\n",
       " Document(metadata={}, page_content='to chunk up. It is the example'),\n",
       " Document(metadata={}, page_content='text'),\n",
       " Document(metadata={}, page_content='for'),\n",
       " Document(metadata={}, page_content='this exercise')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#首选分隔符是\\n\\n，然后是\\n，接着是空格，最后是空字符串\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=35,\n",
    "    chunk_overlap=4,\n",
    ")\n",
    "\n",
    "text = \"This is\\n\\n the\\n\\n text\\n I would like\\n to chunk up. It is the example text\\n for\\n\\n this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ae3641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='text\\n for\\n this exercise')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"text\\n for\\n\\n this exercise\"\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5460b64",
   "metadata": {},
   "source": [
    "以上能看出，先对\\n\\n分割，然后组合chunk，然后对剩余长于chunk_size的进行\\n分割再组合，然后是‘ ’，最后再分割‘’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e10b9",
   "metadata": {},
   "source": [
    "对代码进行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cdad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91cca545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6887d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \\n#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'}, page_content='Implementations of Markdown are available for over a dozen programming languages.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_document = \"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \\n\\n ## Rise and divergence \\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \\n\\n #### Standardization \\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9e0609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'History'}, page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='## Rise and divergence  \\nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \\nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}, page_content='#### Standardization  \\nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.'),\n",
       " Document(metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'}, page_content='## Implementations  \\nImplementations of Markdown are available for over a dozen programming languages.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "# Char-level splits\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(md_header_splits)\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a44c3",
   "metadata": {},
   "source": [
    "可以看出上面再次分割了内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ed70b",
   "metadata": {},
   "source": [
    "根据语义分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b35248",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../state_of_the_union.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This is a long document we can split up.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../state_of_the_union.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     state_of_the_union \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m SemanticChunker(OpenAIEmbeddings())\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../state_of_the_union.txt'"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# This is a long document we can split up.\n",
    "with open(\"../../state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "    \n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "#根据语义进行分割，本质上要用embeddings模型来计算相似度，高于某个阈值的邻域chunk可以合并到一起"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ef274",
   "metadata": {},
   "source": [
    "嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195bbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "# Ensure you have the environment variable set for the API key\n",
    "os.environ[\"OpenAI_API_BASE\"] = \"https://api.zhizengzeng.com/v1\"\n",
    "os.environ[\"OpenAI_API_KEY\"] = \"sk-zk22167acdc980aa5bd5dd04774d59f30f0684991a3a2fb2\"\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    chunk_size=1000,  # Optional, default is 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d277fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "        \"你好吗\",\n",
    "        \"你的名字是什么\",\n",
    "        \"我的肚子好痛啊\",\n",
    "        \"肠胃不舒服\",\n",
    "        \"我在吃东西\"\n",
    "    ]\n",
    "embeddings = embeddings_model.embed_documents(texts)\n",
    "\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df572d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"我的肚子好痛啊\"与\"你好吗\"的语义相似度为：0.3594570606893717\n",
      "\"我的肚子好痛啊\"与\"你的名字是什么\"的语义相似度为：0.23817256999431716\n",
      "\"我的肚子好痛啊\"与\"我的肚子好痛啊\"的语义相似度为：1.0\n",
      "\"我的肚子好痛啊\"与\"肠胃不舒服\"的语义相似度为：0.55998006426397\n",
      "\"我的肚子好痛啊\"与\"我在吃东西\"的语义相似度为：0.4246934047589308\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.asarray(x)\n",
    "    norms = np.sum(np.multiply(x, x))\n",
    "    norms = np.sqrt(norms)\n",
    "    return x / norms\n",
    "\n",
    "for i in range(5):\n",
    "    similarity = np.dot(normalize(embeddings[2]), normalize(embeddings[i]))\n",
    "    print(f'\"{texts[2]}\"与\"{texts[i]}\"的语义相似度为：{similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f2a91",
   "metadata": {},
   "source": [
    "Embedding也可以Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fd677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, store, namespace=embeddings_model.tiktoken_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2c5ff2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\embeddings\\cache.py:125\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of texts.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    The method first checks the cache for the embeddings.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m        A list of embeddings for the given texts.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     vectors: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_embedding_store\u001b[38;5;241m.\u001b[39mmget(\n\u001b[0;32m    126\u001b[0m         texts\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     all_missing_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    129\u001b[0m         i \u001b[38;5;28;01mfor\u001b[39;00m i, vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vectors) \u001b[38;5;28;01mif\u001b[39;00m vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     ]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m missing_indices \u001b[38;5;129;01min\u001b[39;00m batch_iterate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, all_missing_indices):\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\storage\\encoder_backed.py:66\u001b[0m, in \u001b[0;36mEncoderBackedStore.mget\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmget\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: Sequence[K]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Optional[V]]:\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     encoded_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_encoder(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[0;32m     67\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mmget(encoded_keys)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_deserializer(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[0;32m     71\u001b[0m     ]\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\storage\\encoder_backed.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmget\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: Sequence[K]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Optional[V]]:\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     encoded_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_encoder(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[0;32m     67\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mmget(encoded_keys)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_deserializer(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[0;32m     71\u001b[0m     ]\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain\\embeddings\\cache.py:36\u001b[0m, in \u001b[0;36m_key_encoder\u001b[1;34m(key, namespace)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_key_encoder\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, namespace: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode a key.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(_hash_string_to_uuid(key))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cached_embedder.embed_documents(texts)\n",
    "#两次调用会让第一次调用的结果被缓存起来，第二次调用会直接从缓存中读取，从而加速第二次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3416d5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CacheBackedEmbeddings.embed_documents of <langchain.embeddings.cache.CacheBackedEmbeddings object at 0x000001D9A57CF650>>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, store, namespace=embeddings_model.tiktoken_model_name\n",
    ")\n",
    "\n",
    "cached_embedder.embed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "139c11a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Either a Redis client or a redis_url must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RedisStore\n\u001b[1;32m----> 3\u001b[0m store \u001b[38;5;241m=\u001b[39m RedisStore()\n\u001b[0;32m      5\u001b[0m cached_embedder \u001b[38;5;241m=\u001b[39m CacheBackedEmbeddings\u001b[38;5;241m.\u001b[39mfrom_bytes_store(\n\u001b[0;32m      6\u001b[0m     embeddings_model, store, namespace\u001b[38;5;241m=\u001b[39membeddings_model\u001b[38;5;241m.\u001b[39mtiktoken_model_name\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m cached_embedder\u001b[38;5;241m.\u001b[39membed_documents\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_community\\storage\\redis.py:74\u001b[0m, in \u001b[0;36mRedisStore.__init__\u001b[1;34m(self, client, redis_url, client_kwargs, ttl, namespace)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither a Redis client or a redis_url with optional client_kwargs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be provided, but not both.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m     )\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redis_url:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither a Redis client or a redis_url must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(client, Redis):\n",
      "\u001b[1;31mValueError\u001b[0m: Either a Redis client or a redis_url must be provided."
     ]
    }
   ],
   "source": [
    "from langchain.storage import RedisStore\n",
    "\n",
    "store = RedisStore()\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings_model, store, namespace=embeddings_model.tiktoken_model_name\n",
    ")\n",
    "\n",
    "cached_embedder.embed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020ec67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
